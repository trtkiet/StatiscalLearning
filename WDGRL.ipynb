{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class WDGRL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Domain critic\n",
    "        self.domain_critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        extracted = self.feature_extractor(x)\n",
    "        class_pred = self.classifier(extracted)\n",
    "        \n",
    "        return class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_data import *\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loop_iterable(iterable):\n",
    "    while True:\n",
    "        yield from iterable\n",
    "\n",
    "def gradient_penalty(critic, h_s, h_t):\n",
    "    # based on: https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py#L116\n",
    "    alpha = torch.rand(h_s.size(0), 1)\n",
    "    differences = h_t - h_s\n",
    "    interpolates = h_s + (alpha * differences)\n",
    "    interpolates = torch.stack([interpolates, h_s, h_t]).requires_grad_()\n",
    "\n",
    "    preds = critic(interpolates)\n",
    "    gradients = grad(preds, interpolates,\n",
    "                     grad_outputs=torch.ones_like(preds),\n",
    "                     retain_graph=True, create_graph=True)[0]\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1)**2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def train_WDGRL(input_dim, hidden_dim, output_dim=2,num_samples=1000, lr=0.001, \n",
    "             batch_size=128, max_epochs=1000000):\n",
    "    \n",
    "    batch_size = min(batch_size, num_samples)\n",
    "    mu = 0\n",
    "    delta = 2\n",
    "    D_train_num = 10\n",
    "    gamma = 10\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    X_s, y_s = gen_data(mu, delta, num_samples, input_dim)\n",
    "    X_t, _ = gen_data(mu + 5, delta, num_samples, input_dim)\n",
    "\n",
    "    with open(f'training_set/source_{input_dim}d.txt', 'w') as f:\n",
    "        for i in range(len(X_s)):\n",
    "            f.write(' '.join(map(str, X_s[i])) + ' ' + str(y_s[i]) + '\\n')\n",
    "\n",
    "    \n",
    "    with open(f'training_set/target_{input_dim}d.txt', 'w') as f:\n",
    "        for i in range(len(X_t)):\n",
    "            f.write(' '.join(map(str, X_t[i])) + '\\n')\n",
    "    # training_set = []\n",
    "    # with open(f'training_set/{input_dim}d.txt', 'r') as f:\n",
    "    #     for line in f:\n",
    "    #         training_set.append(list(map(float, line.strip().split())))\n",
    "\n",
    "    X_s = torch.tensor(X_s, dtype=torch.float32)\n",
    "    y_s = torch.tensor(y_s, dtype=torch.float32)\n",
    "    source_dataset = TensorDataset(X_s, y_s)\n",
    "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    X_t = torch.tensor(X_t, dtype=torch.float32)\n",
    "    target_dataset = TensorDataset(X_t)\n",
    "    target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # model = WDGRL(input_dim, hidden_dim, output_dim)\n",
    "    # critic_optim = optim.Adam(model.domain_critic.parameters(), lr=lr)\n",
    "    # feature_optim = optim.Adam(model.feature_extractor.parameters(), lr=lr)\n",
    "    # clf_optim = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    # clf_criterion = nn.CrossEntropyLoss()\n",
    "    # # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # epoch_losses = []\n",
    "    # epoch_loss = torch.inf\n",
    "    # for i in range(max_epochs):\n",
    "    #     batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_loader))\n",
    "\n",
    "    #     total_loss = 0\n",
    "    #     for _ in range(D_train_num):\n",
    "    #         (xs, ys), (xt) = next(batch_iterator)\n",
    "    #         xt = xt[0]\n",
    "           \n",
    "    #         h_s = model.feature_extractor(xs)\n",
    "    #         h_t = model.feature_extractor(xt)\n",
    "\n",
    "    #         for _ in range(D_train_num):\n",
    "    #             gp = gradient_penalty(model.domain_critic, h_s, h_t)\n",
    "\n",
    "    #             critic_s = model.domain_critic(h_s)\n",
    "    #             critic_t = model.domain_critic(h_t)\n",
    "    #             watterstein_distance = critic_s.mean() - critic_t.mean()\n",
    "    #             critic_cost = -watterstein_distance + gamma * gp\n",
    "\n",
    "    #             critic_optim.zero_grad()\n",
    "    #             critic_cost.backward(retain_graph=True)\n",
    "    #             critic_optim.step()\n",
    "            \n",
    "    #         loss = clf_criterion(h_s, ys.long())\n",
    "    #         clf_optim.zero_grad()\n",
    "    #         loss.backward(retain_graph=True)\n",
    "    #         clf_optim.step()\n",
    "\n",
    "    #         critic_s = model.domain_critic(h_s)\n",
    "    #         critic_t = model.domain_critic(h_t)\n",
    "    #         watterstein_distance = critic_s.mean() - critic_t.mean()\n",
    "    #         critic_loss = -watterstein_distance + gamma * gp\n",
    "\n",
    "    #         feature_extractor_loss = critic_loss + loss\n",
    "    #         feature_optim.zero_grad()\n",
    "    #         feature_extractor_loss.backward()\n",
    "    #         feature_optim.step()\n",
    "    #         print(f\"Epoch {len(epoch_losses)}\\t\\tLoss: {loss:.4f}\")\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    # print(\"Final loss: {:.4f}\".format(epoch_loss))\n",
    "    # if epoch_loss > 0.05:\n",
    "    #     print(\"Warning: Loss did not converge to desired value\")\n",
    "\n",
    "    # plt.plot(range(1, len(epoch_losses)+1), epoch_losses)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title(f'Training Loss ({input_dim}D -> {hidden_dim}D)')\n",
    "    # plt.savefig(f\"loss/{input_dim}d_{hidden_dim}d.png\")\n",
    "    # plt.close()\n",
    "    # # Save the trained model\n",
    "    # torch.save(model.state_dict(), f\"model/{input_dim}d_{hidden_dim}d.pth\")\n",
    "    # print(f\"Model saved as 'model/{input_dim}d_{hidden_dim}d.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12636\\3275390451.py\", line 1, in <module>\n",
      "    train_WDGRL(5, 3, 2)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12636\\1557804343.py\", line 81, in train_WDGRL\n",
      "    gp = gradient_penalty(model.domain_critic, h_s, h_t)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12636\\1557804343.py\", line 19, in gradient_penalty\n",
      "    gradients = grad(preds, interpolates,\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 496, in grad\n",
      "    result = _engine_run_backward(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:115.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: \n",
      "\n",
      "Previous calculation was induced by AddmmBackward0. Traceback of forward call that induced the previous calculation:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12636\\3275390451.py\", line 1, in <module>\n",
      "    train_WDGRL(5, 3, 2)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12636\\1557804343.py\", line 81, in train_WDGRL\n",
      "    gp = gradient_penalty(model.domain_critic, h_s, h_t)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12636\\1557804343.py\", line 18, in gradient_penalty\n",
      "    preds = critic(interpolates)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:123.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 3]], which is output 0 of AsStridedBackward0, is at version 11; expected version 10 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_WDGRL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 104\u001b[0m, in \u001b[0;36mtrain_WDGRL\u001b[1;34m(input_dim, hidden_dim, output_dim, num_samples, lr, batch_size, max_epochs)\u001b[0m\n\u001b[0;32m    102\u001b[0m feature_extractor_loss \u001b[38;5;241m=\u001b[39m critic_loss \u001b[38;5;241m+\u001b[39m loss\n\u001b[0;32m    103\u001b[0m feature_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 104\u001b[0m \u001b[43mfeature_extractor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m feature_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(epoch_losses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 3]], which is output 0 of AsStridedBackward0, is at version 11; expected version 10 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DomainDiscriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DomainDiscriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class WDGRL:\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, lambda_wd=1.0, n_critic=5):\n",
    "        self.feature_extractor = FeatureExtractor(input_dim, hidden_dim)\n",
    "        self.classifier = Classifier(hidden_dim, num_classes)\n",
    "        self.domain_discriminator = DomainDiscriminator(hidden_dim)\n",
    "        \n",
    "        self.lambda_wd = lambda_wd\n",
    "        self.n_critic = n_critic\n",
    "        \n",
    "        # Optimizers\n",
    "        self.opt_feat_class = optim.Adam(list(self.feature_extractor.parameters()) + \n",
    "                                       list(self.classifier.parameters()))\n",
    "        self.opt_disc = optim.Adam(self.domain_discriminator.parameters())\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_wasserstein_distance(self, source_features, target_features):\n",
    "        # Compute Wasserstein distance using domain discriminator\n",
    "        source_pred = self.domain_discriminator(source_features)\n",
    "        target_pred = self.domain_discriminator(target_features)\n",
    "        return torch.mean(source_pred) - torch.mean(target_pred)\n",
    "    \n",
    "    def train_step(self, source_data, source_labels, target_data):\n",
    "        # Extract features\n",
    "        source_features = self.feature_extractor(source_data)\n",
    "        target_features = self.feature_extractor(target_data)\n",
    "        \n",
    "        # Train domain discriminator\n",
    "        for _ in range(self.n_critic):\n",
    "            self.opt_disc.zero_grad()\n",
    "            wasserstein_distance = self.compute_wasserstein_distance(source_features.detach(),\n",
    "                                                                   target_features.detach())\n",
    "            \n",
    "            # Gradient penalty\n",
    "            alpha = torch.rand(source_features.size(0), 1)\n",
    "            interpolates = alpha * source_features.detach() + (1 - alpha) * target_features.detach()\n",
    "            interpolates.requires_grad = True\n",
    "            disc_interpolates = self.domain_discriminator(interpolates)\n",
    "            gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                          grad_outputs=torch.ones_like(disc_interpolates),\n",
    "                                          create_graph=True, retain_graph=True)[0]\n",
    "            gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            \n",
    "            disc_loss = -wasserstein_distance + 10 * gradient_penalty\n",
    "            disc_loss.backward()\n",
    "            self.opt_disc.step()\n",
    "        \n",
    "        # Train feature extractor and classifier\n",
    "        self.opt_feat_class.zero_grad()\n",
    "        \n",
    "        # Classification loss\n",
    "        source_logits = self.classifier(source_features)\n",
    "        class_loss = self.criterion(source_logits, source_labels)\n",
    "        \n",
    "        # Domain adaptation loss\n",
    "        wasserstein_distance = self.compute_wasserstein_distance(source_features, target_features)\n",
    "        total_loss = class_loss - self.lambda_wd * wasserstein_distance\n",
    "        \n",
    "        total_loss.backward()\n",
    "        self.opt_feat_class.step()\n",
    "        \n",
    "        return {\n",
    "            'classification_loss': class_loss.item(),\n",
    "            'wasserstein_distance': wasserstein_distance.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        self.classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(x)\n",
    "            logits = self.classifier(features)\n",
    "        return torch.argmax(logits, dim=1)\n",
    "\n",
    "# Example usage\n",
    "def train_wdgrl(source_loader, target_loader, input_dim, hidden_dim, num_classes, num_epochs=100):\n",
    "    model = WDGRL(input_dim=input_dim, \n",
    "                  hidden_dim=hidden_dim,\n",
    "                  num_classes=num_classes)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for (source_data, source_labels), (target_data) in zip(source_loader, target_loader):\n",
    "            losses = model.train_step(source_data, source_labels, target_data)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}\")\n",
    "                print(f\"Classification Loss: {losses['classification_loss']:.4f}\")\n",
    "                print(f\"Wasserstein Distance: {losses['wasserstein_distance']:.4f}\")\n",
    "                print(f\"Total Loss: {losses['total_loss']:.4f}\")\n",
    "                print(\"-\" * 50)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
